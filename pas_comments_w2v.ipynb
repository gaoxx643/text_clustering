{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "import tqdm\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "alls = pd.read_csv('raw_data.txt', encoding='utf-8', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>rate_detail</th>\n",
       "      <th>rate_num</th>\n",
       "      <th>detaillens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.152990e+18</td>\n",
       "      <td>师傅人特别好，本人比图片年轻好多，也幽默，祝他好运，开心?！</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.766720e+17</td>\n",
       "      <td>歌不好听，不要觉得很奇葩，而是他听的歌让人害怕。</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       order_id                     rate_detail  rate_num  detaillens\n",
       "0  1.152990e+18  师傅人特别好，本人比图片年轻好多，也幽默，祝他好运，开心?！       3.0        30.0\n",
       "1  5.766720e+17        歌不好听，不要觉得很奇葩，而是他听的歌让人害怕。       1.0        24.0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alls.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "alls[['rate_detail']].to_csv('w2v.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "forWord2vec = read('w2v.txt')\n",
    "with open ('./result/split_w2v.txt', 'w') as f:\n",
    "    f.write(forWord2vec)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# -*- coding=utf-8 -*-\n",
    "# File Name: word2vec.py\n",
    "# Author: RuanTao\n",
    "# mail: ruantao@jd.com\n",
    "# Created Time: Tue 26 Feb 2019 09:34:01 PM CST\n",
    "#=============================================================\n",
    "#!/usr/bin/python\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import logging\n",
    "from gensim.models import word2vec\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
    "\n",
    "# 训练模型\n",
    "def getModel(sequence_file,model_file,vector_file,vocab_file):\n",
    "    sequence = word2vec.LineSentence(sequence_file)\n",
    "    model = word2vec.Word2Vec(sequence,size=300,sg=0,hs=0,window=5,negative=5,min_count=1)\n",
    "    model.save(model_file)\n",
    "    model.wv.save_word2vec_format(vector_file,vocab_file)\n",
    "\n",
    "# 二维空间示意图\n",
    "def plot_with_labels(matrix_2d,labels,filename='tsne.png'):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "\n",
    "    plt.switch_backend('agg')\n",
    "    plt.figure(figsize=(20,12))\n",
    "    plt.title(u\"中文词向量可视化显示\",loc='center',fontsize=30)\n",
    "\n",
    "    for i,label in enumerate(labels):\n",
    "        x,y = matrix_2d[i,:]\n",
    "        plt.scatter(x,y)\n",
    "        plt.annotate(label,xy=(x,y),xytext=(5,2),textcoords='offset points',ha='right',va='bottom')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "def visualize_embedding(model_file):\n",
    "    import numpy as np\n",
    "\n",
    "    model = word2vec.Word2Vec.load(model_file)\n",
    "    # 选出2000个单词\n",
    "    count = 2000\n",
    "    word_vectors_matrix = np.ndarray(shape=(count,300),dtype='float32')\n",
    "    labels = []\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        if len(word) > 1:\n",
    "            word_vectors_matrix[i] = model[word]\n",
    "            labels.append(word)\n",
    "            i += 1\n",
    "            if i == count:\n",
    "                break\n",
    "    print(\"word_vectors_matrix shape is: \",word_vectors_matrix.shape)\n",
    "\n",
    "    import sklearn.manifold as ts\n",
    "    tsne = ts.TSNE(n_components=2,random_state=0)\n",
    "    #tsne = ts.TSNE(perplexity=5,n_components=2,init='pca',n_iter=2500,random_state=0)\n",
    "    word_vectors_matrix_2d = tsne.fit_transform(word_vectors_matrix)\n",
    "    print(\"word_vectors_matrix_2d shape is: \",word_vectors_matrix_2d.shape)\n",
    "    \n",
    "    plot_with_labels(word_vectors_matrix_2d,labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-20 11:30:39,393 : INFO : collecting all words and their counts\n",
      "2019-03-20 11:30:39,396 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-03-20 11:30:39,566 : INFO : PROGRESS: at sentence #10000, processed 134195 words, keeping 12698 word types\n",
      "2019-03-20 11:30:39,694 : INFO : PROGRESS: at sentence #20000, processed 266077 words, keeping 18770 word types\n",
      "2019-03-20 11:30:39,820 : INFO : PROGRESS: at sentence #30000, processed 397708 words, keeping 23484 word types\n",
      "2019-03-20 11:30:39,964 : INFO : PROGRESS: at sentence #40000, processed 527151 words, keeping 27321 word types\n",
      "2019-03-20 11:30:40,117 : INFO : PROGRESS: at sentence #50000, processed 660842 words, keeping 30837 word types\n",
      "2019-03-20 11:30:40,268 : INFO : PROGRESS: at sentence #60000, processed 793367 words, keeping 33965 word types\n",
      "2019-03-20 11:30:40,405 : INFO : PROGRESS: at sentence #70000, processed 925015 words, keeping 36752 word types\n",
      "2019-03-20 11:30:40,530 : INFO : PROGRESS: at sentence #80000, processed 1053665 words, keeping 39359 word types\n",
      "2019-03-20 11:30:40,584 : INFO : collected 40115 word types from a corpus of 1094246 raw words and 83086 sentences\n",
      "2019-03-20 11:30:40,585 : INFO : Loading a fresh vocabulary\n",
      "2019-03-20 11:30:40,960 : INFO : effective_min_count=1 retains 40115 unique words (100% of original 40115, drops 0)\n",
      "2019-03-20 11:30:40,963 : INFO : effective_min_count=1 leaves 1094246 word corpus (100% of original 1094246, drops 0)\n",
      "2019-03-20 11:30:41,343 : INFO : deleting the raw counts dictionary of 40115 items\n",
      "2019-03-20 11:30:41,344 : INFO : sample=0.001 downsamples 56 most-common words\n",
      "2019-03-20 11:30:41,346 : INFO : downsampling leaves estimated 952307 word corpus (87.0% of prior 1094246)\n",
      "2019-03-20 11:30:41,559 : INFO : estimated required memory for 40115 words and 300 dimensions: 116333500 bytes\n",
      "2019-03-20 11:30:41,561 : INFO : resetting layer weights\n",
      "2019-03-20 11:30:42,982 : INFO : training model with 3 workers on 40115 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-03-20 11:30:44,007 : INFO : EPOCH 1 - PROGRESS: at 30.86% examples, 290179 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-20 11:30:45,011 : INFO : EPOCH 1 - PROGRESS: at 69.94% examples, 330878 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-20 11:30:45,907 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-20 11:30:45,911 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-20 11:30:45,947 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-20 11:30:45,948 : INFO : EPOCH - 1 : training on 1094246 raw words (951694 effective words) took 3.0s, 321694 effective words/s\n",
      "2019-03-20 11:30:46,966 : INFO : EPOCH 2 - PROGRESS: at 25.27% examples, 241148 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-20 11:30:47,984 : INFO : EPOCH 2 - PROGRESS: at 56.37% examples, 265807 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-20 11:30:49,005 : INFO : EPOCH 2 - PROGRESS: at 93.05% examples, 290838 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-20 11:30:49,178 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-20 11:30:49,187 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-20 11:30:49,189 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-20 11:30:49,191 : INFO : EPOCH - 2 : training on 1094246 raw words (952185 effective words) took 3.2s, 294365 effective words/s\n",
      "2019-03-20 11:30:50,236 : INFO : EPOCH 3 - PROGRESS: at 37.25% examples, 344722 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-20 11:30:51,242 : INFO : EPOCH 3 - PROGRESS: at 80.95% examples, 379308 words/s, in_qsize 1, out_qsize 0\n",
      "2019-03-20 11:30:51,629 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-20 11:30:51,649 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-20 11:30:51,658 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-20 11:30:51,660 : INFO : EPOCH - 3 : training on 1094246 raw words (952068 effective words) took 2.5s, 387522 effective words/s\n",
      "2019-03-20 11:30:52,667 : INFO : EPOCH 4 - PROGRESS: at 42.77% examples, 407665 words/s, in_qsize 0, out_qsize 0\n",
      "2019-03-20 11:30:53,671 : INFO : EPOCH 4 - PROGRESS: at 82.78% examples, 394413 words/s, in_qsize 0, out_qsize 0\n",
      "2019-03-20 11:30:54,139 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-20 11:30:54,147 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-20 11:30:54,154 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-20 11:30:54,156 : INFO : EPOCH - 4 : training on 1094246 raw words (952629 effective words) took 2.5s, 382345 effective words/s\n",
      "2019-03-20 11:30:55,170 : INFO : EPOCH 5 - PROGRESS: at 42.77% examples, 404636 words/s, in_qsize 0, out_qsize 0\n",
      "2019-03-20 11:30:56,172 : INFO : EPOCH 5 - PROGRESS: at 93.05% examples, 440932 words/s, in_qsize 0, out_qsize 0\n",
      "2019-03-20 11:30:56,326 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-20 11:30:56,329 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-20 11:30:56,344 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-20 11:30:56,345 : INFO : EPOCH - 5 : training on 1094246 raw words (952097 effective words) took 2.2s, 435750 effective words/s\n",
      "2019-03-20 11:30:56,347 : INFO : training on a 5471230 raw words (4760673 effective words) took 13.4s, 356273 effective words/s\n",
      "2019-03-20 11:30:56,349 : INFO : saving Word2Vec object under ./result/zh_cars_split_cbow_ng_win5_min1_size300.model, separately None\n",
      "2019-03-20 11:30:56,351 : INFO : storing np array 'vectors' to ./result/zh_cars_split_cbow_ng_win5_min1_size300.model.wv.vectors.npy\n",
      "2019-03-20 11:30:56,535 : INFO : not storing attribute vectors_norm\n",
      "2019-03-20 11:30:56,537 : INFO : storing np array 'syn1neg' to ./result/zh_cars_split_cbow_ng_win5_min1_size300.model.trainables.syn1neg.npy\n",
      "2019-03-20 11:30:56,695 : INFO : not storing attribute cum_table\n",
      "2019-03-20 11:30:56,824 : INFO : saved ./result/zh_cars_split_cbow_ng_win5_min1_size300.model\n",
      "2019-03-20 11:30:56,826 : INFO : storing vocabulary in ./result/zh_cars_split_cbow_ng_win5_min1_size300_vocab.txt\n",
      "2019-03-20 11:30:56,960 : INFO : storing 40115x300 projection weights into ./result/zh_cars_split_cbow_ng_win5_min1_size300_vector.txt\n",
      "2019-03-20 11:31:12,605 : INFO : loading Word2Vec object from ./result/zh_cars_split_cbow_ng_win5_min1_size300.model\n",
      "2019-03-20 11:31:12,817 : INFO : loading wv recursively from ./result/zh_cars_split_cbow_ng_win5_min1_size300.model.wv.* with mmap=None\n",
      "2019-03-20 11:31:12,819 : INFO : loading vectors from ./result/zh_cars_split_cbow_ng_win5_min1_size300.model.wv.vectors.npy with mmap=None\n",
      "2019-03-20 11:31:12,897 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-03-20 11:31:12,903 : INFO : loading vocabulary recursively from ./result/zh_cars_split_cbow_ng_win5_min1_size300.model.vocabulary.* with mmap=None\n",
      "2019-03-20 11:31:12,905 : INFO : loading trainables recursively from ./result/zh_cars_split_cbow_ng_win5_min1_size300.model.trainables.* with mmap=None\n",
      "2019-03-20 11:31:12,908 : INFO : loading syn1neg from ./result/zh_cars_split_cbow_ng_win5_min1_size300.model.trainables.syn1neg.npy with mmap=None\n",
      "2019-03-20 11:31:12,997 : INFO : setting ignored attribute cum_table to None\n",
      "2019-03-20 11:31:12,998 : INFO : loaded ./result/zh_cars_split_cbow_ng_win5_min1_size300.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_vectors_matrix shape is:  (2000, 300)\n",
      "word_vectors_matrix_2d shape is:  (2000, 2)\n"
     ]
    }
   ],
   "source": [
    "# 训练word2vec\n",
    "sequence_file = \"./result/split_w2v.txt\"  # 要训练的原文件\n",
    "model_file = \"./result/zh_cars_split_cbow_ng_win5_min1_size300.model\" #保存的模型文件\n",
    "vector_file = \"./result/zh_cars_split_cbow_ng_win5_min1_size300_vector.txt\" #保存的词向量文件\n",
    "vocab_file = \"./result/zh_cars_split_cbow_ng_win5_min1_size300_vocab.txt\" # 保存的词表文件\n",
    "getModel(sequence_file,model_file,vector_file,vocab_file)\n",
    "# visualize_embedding(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
